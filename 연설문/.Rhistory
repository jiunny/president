mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown)))) %>%
filter(diff!=1)
weath %>%
mutate(rown = row.names(.)) %>%
arrange(date) %>%
filter(구분=='해제') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown)))) %>%
filter(diff!=1) %>%
pull(rown) -> uniq_release
weath %>%
mutate(rown = row.names(.)) %>%
arrange(date) %>%
filter(구분=='발표') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown))))
weath %>%
mutate(rown = row.names(.)) %>%
arrange(date)
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='해제') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown)))) %>%
filter(diff!=1) %>%
pull(rown) -> uniq_release
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='발표') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown))))
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='발표') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown)))) %>%
filter(diff!=1)
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='발표') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown)))) %>%
filter(diff!=1) %>%
pull(rown) -> uniq_start
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='해제') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown)))) %>%
filter(diff!=1) %>%
pull(rown) -> uniq_end
uniq_end
uniq_start
sort(c(as.numeric(uniq_end),as.numeric(uniq_start)))
weath %>%
sort(c(as.numeric(uniq_end),as.numeric(uniq_start)))
sort(c(as.numeric(uniq_end),as.numeric(uniq_start)))
weath %>%
slice(sort(c(as.numeric(uniq_end),as.numeric(uniq_start))))
weath %>% arrange(date)
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='해제') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,-1,default = first(rown))))
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='해제') %>%
mutate(diff=as.numeric(rown)-as.numeric(lead(rown,1,default = first(rown))))
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='해제') %>%
mutate(diff=as.numeric(rown)-as.numeric(lead(rown,1,default = first(rown)))) %>%
filter(diff!=-1)
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='해제') %>%
mutate(diff=as.numeric(rown)-as.numeric(lead(rown,1,default = first(rown)))) %>%
filter(diff!=-1) %>%
pull(rown) -> uniq_end
weath %>%
arrange(date) %>%
mutate(rown = row.names(.)) %>%
filter(구분=='발표') %>%
mutate(diff=as.numeric(rown)-as.numeric(lag(rown,1,default = first(rown)))) %>%
filter(diff!=1) %>%
pull(rown) -> uniq_start
weath %>%
slice(sort(c(as.numeric(uniq_end),as.numeric(uniq_start))))
weath %>% arrange(date)
library(data.table)
library(dplyr)
library(plotly)
library(ggplot2)
library(purrr)
library(stringr)
library(plyr)
library(flsa)
train_DTM <- fread("C:\\Users\\DKU\\Desktop\\fused_lasso\\train_DTM.csv") %>%
as.data.frame()
test_DTM <- fread("C:\\Users\\DKU\\Desktop\\fused_lasso\\test_DTM.csv") %>%
as.data.frame()
train_DTM <- train_DTM[,-1]
test_DTM <- test_DTM[,-1]
train_DTM$y <- gsub("\\['","",train_DTM$y)
train_DTM$y <- gsub("\'\\]","",train_DTM$y)
train_DTM$y <- gsub("\', \'",",",train_DTM$y)
test_DTM$y <- gsub("\\['","",test_DTM$y)
test_DTM$y <- gsub("\'\\]","",test_DTM$y)
test_DTM$y <- gsub("\', \'",",",test_DTM$y)
y_name <- names(sort(table(train_DTM$y))[sort(table(train_DTM$y))>3])
train_DTM %>%
filter(y %in% y_name) -> train_DTM
train_DTM$num_y <- as.numeric(as.factor(train_DTM$y))
train_DTM %>%
arrange(num_y) %>%
select(-docn) -> train_DTM
y_name <- names(sort(table(test_DTM$y))[sort(table(test_DTM$y))>3])
test_DTM %>%
filter(y %in% y_name) -> test_DTM
test_DTM$num_y <- as.numeric(as.factor(test_DTM$y))
test_DTM %>%
arrange(num_y) %>%
select(-docn) -> test_DTM
category <- sort(table(train_DTM$y),decreasing = T)[1:10]
uniq_cate <- unique(train_DTM$y)
setClass(Class="select_words",
representation(
flsa_result="list",
cate_theta_min="numeric",
cate_theta_max = 'numeric',
cate_theta_diff = 'numeric',
order_words = 'character',
chiq_words = 'character'
)
)
fn_select_word_by_theta <- function(y_cate = category[1],
uniq_category = uniq_cate,
train_dtm = train_DTM,
train_y = train_DTM$y,
chiq_words = words,
lamb2 = 30){
#원하는 카테고리 가운데에 위치하게 하기위한 작업
uniq_y <- grepl(names(y_cate),unique(train_y))
not_y <- length(uniq_category) - sum(uniq_y)
y_order <- c(uniq_category[uniq_y==F][1:round(not_y/2)],
uniq_category[uniq_y],
uniq_category[uniq_y==F][(ceiling(not_y/2)+1):not_y])
train_dtm %>%
arrange(factor(y,levels=y_order)) -> order_train_dtm
#flsa 계산
flsa_result <- list() #단어별 flsa theta 값
cate_theta_min <- c() #카테고리에 속하는 theta의 min
cate_theta_max <- c() #카테고리에 속하지 않는 theta의 max
for(i in 1:length(chiq_words)){
cocoa_flsa_DTM <- order_train_dtm %>%
pull(chiq_words[i]) %>%
flsa(lambda2 = lamb2)
cocoa_rown <- which(grepl(names(y_cate),order_train_DTM$y))
flsa_result[[i]] <- as.data.frame(t(cocoa_flsa_DTM)) %>%
mutate(rownum = row.names(.),
colorn = as.factor(ifelse(rownum %in% cocoa_rown,'Yes','No')))
cate_theta_min[i] <- min(flsa_result[[i]][flsa_result[[i]]$colorn=='Yes',"V1"])
cate_theta_max[i] <- max(flsa_result[[i]][flsa_result[[i]]$colorn=='No',"V1"])
if(cate_theta_min[i]==cate_theta_max[i]){
cate_theta_min[i] <- sort(unique(flsa_result[[i]][flsa_result[[i]]$colorn=='Yes',"V1"]))[2]
cate_theta_max[i] <- sort(unique(flsa_result[[i]][flsa_result[[i]]$colorn=='No',"V1"]),decreasing = T)[2]
}
}
names(flsa_result) <- chiq_words
cate_diff <- abs(cate_theta_max - cate_theta_min)
cate_word <- chiq_words[which(!is.na(cate_diff))]
# max-min 가장 큰 단어 순으로 정렬
order_chiq_words <- cate_word[order(na.omit(cate_diff),decreasing = T)]
# return(flsa_result,cate_theta_min,cate_theta_max,order_chiq_words)
return(new('select_words',
flsa_result = flsa_result,
cate_theta_min = cate_theta_min,
cate_theta_max = cate_theta_max,
cate_theta_diff = abs(cate_theta_max - cate_theta_min),
order_words = order_chiq_words,
chiq_words = chiq_words))
}
final_chiq <- readRDS("C:/Users/DKU/Desktop/final_chiq.rds")
final_chiq <- readRDS("C:/Users/DKU/Desktop/fused_lasso/final_chiq.rds")
final_theta <- readRDS("C:/Users/DKU/Desktop/fused_lasso/final_theta.rds")
final_theta
final_chiq
library(randomForest)
train_DTM %>%
select(final_theta,y) -> final_train
final_train$y
final_train$y <- as.factor(final_train$y)
final_train$y
library(randomForest)
rf.model <- randomForest(y~., data = final_train, na.action = na.omit)
rf.model$predicted
sort(rf.model$importance)
row.names(rf.model$importance)[order(rf.model$importance)]
test_DTM %>%
select(final_theta,y) -> final_test
final_test$y <- as.factor(final_test$y)
rf.pred <- predict(rf.model, newdata = final_test, type='class')
table(rf.pred, final_test$y)
sum(as.character(rf.pred)==as.character(final_test$y))/nrow(final_test)*100 #80.5712
train_DTM$y <- as.factor(train_DTM$y)
final_chiq
rf.raw.model <- randomForest(y~., data = train_DTM %>% select(final_chiq,y), na.action = na.omit)
rf.raw.model$predicted
row.names(rf.raw.model$importance)[order(rf.raw.model$importance)]
test_DTM$y <- as.factor(test_DTM$y)
rf.raw.pred <- predict(rf.model, newdata = test_DTM %>% select(final_chiq,y), type='class')
table(rf.raw.pred, final_test$y)
table(rf.pred, final_test$y)
sum(as.character(rf.raw.pred)==as.character(test_DTM$y))/nrow(final_test)*100 #80.5712
table(rf.raw.pred, final_test$y)
row.names(rf.raw.model$importance)[order(rf.raw.model$importance)]
row.names(rf.raw.model$importance)[order(rf.raw.model$importance,decreasing = T)]
row.names(rf.raw.model$importance)[order(rf.raw.model$importance,decreasing = T)][1:100]
row.names(rf.model$importance)[order(rf.model$importance,decreasing = T)][1:100]
rf.model$err.rate
plot(rf.model)
rf.model$mtry
rf.model$mtree
rf.model$ntree
final_train
set.seed(0921)
rf.model <- randomForest(y~., data = final_train, na.action = na.omit)
row.names(rf.model$importance)[order(rf.model$importance,decreasing = T)][1:100]
rf.pred <- predict(rf.model, newdata = final_test, type='class')
sum(as.character(rf.pred)==as.character(final_test$y))/nrow(final_test)*100 #86.00161
set.seed(0921)
rf.raw.model <- randomForest(y~., data = train_DTM %>% select(final_chiq,y), na.action = na.omit)
train_DTM$balances
table(train_DTM$balances)
table(rf.pred, final_test$y)
as.data.frame.table(rf.pred, final_test$y)
table(rf.pred, final_test$y)
as.matrix(table(rf.pred, final_test$y))
as.matrix(table(rf.pred, final_test$y)) %>% write.csv(.,"C:/Users/Desktop/flsa_acc.csv")
as.matrix(table(rf.pred, final_test$y)) %>% write.csv(.,"C:/Users/DKU/Desktop/flsa_acc.csv")
table(train_DTM$y)
table(test_DTM$y)
row.names(rf.raw.model$importance)[order(rf.raw.model$importance,decreasing = T)][1:100]
row.names(rf.model$importance)[order(rf.model$importance,decreasing = T)][1:100]
row.names(rf.model$importance)[order(rf.model$importance,decreasing = T)][1:100]
row.names(rf.raw.model$importance)[order(rf.raw.model$importance,decreasing = T)][1:100]
final_chiq
final_theta
length(final_chiq)
my.text.location <- "C:/Users/DKU/Desktop/연설문"
setwd(my.text.location)
file_ls <- list.files(my.text.location)
library(rJava)
#install.packages("multilinguer")
library(multilinguer)
library(KoNLP)
library(tm)    # 영문 텍스트 마이닝
library(wordcloud)
install.packages('glue')
#install_jdk()
#install.packages(c('stringr', 'hash', 'tau', 'Sejong', 'RSQLite', 'devtools'), type = "binary")
#install.packages("remotes")
#remotes::install_github('haven-jeon/KoNLP', upgrade = "never", INSTALL_opts=c("--no-multiarch"))
library(KoNLP)
library(wordcloud)
useSejongDic()
library(data.table)
first <- fread('이명박.txt',encoding='UTF-8')
head(first)
first <- read.table('이명박.txt',encoding='UTF-8')
head(first)
first <- read.table('김대중.txt',encoding='UTF-8')
first <- read.table('김대중.txt',encoding='UTF-8')
first <- read.table('김대중.txt')
first <- fread('김대중.txt',header=F,stringAsFactors=F)
first <- read.table('김대중.txt',header=FALSE, fill=T)
first
mypaper=VCorpus(DirSource(my.text.location))
mypaper[[1]]
gsub("[[:punct:]]", "", mypaper$김대중.txt)
mypaper$김대중.txt
mypaper[[1]]$content
unlist(mypaper[[1]]$content)
length(mypaper[[1]]$content)
inspect(mypaper)
tdm <- DocumentTermMatrix(mypaper, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
tdm
as.matrix(tdm)[1:5,1:5]
Encoding(tdm$dimnames$Terms) = 'UTF-8'
as.matrix(tdm)[1:5,1:5]
mypaper=VCorpus(DirSource(my.text.location))
inspect(mypaper)
tdm <- DocumentTermMatrix(mypaper, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
Encoding(tdm$dimnames$Terms) = 'UTF-8'
as.matrix(tdm)[1:5,1:5]
mypaper=VCorpus(DirSource(my.text.location))
mypaper[[1]]$content
mypaper[[2]]$content
first <- readLines('김대중.txt',encoding='UTF-8')
first
head(first)
str_replace_all(first, "[\r\n]" , "")
library(stringr)
str_replace_all(first, "[\r\n]" , "")
first[which(first!='')]
file_ls <- list.files(my.text.location)
first <- readLines('김대중.txt',encoding='UTF-8')
first <- Corpus(VectorSource(first))
tdm <- DocumentTermMatrix(first, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
extractNoun(first)
str_split(first," ")
first <- readLines('김대중.txt',encoding='UTF-8')
str_split(first," ")
str_split(gsub("[[:punct:]]", "", first)," ")
first <- str_split(gsub("[[:punct:]]", "", first)," ")
gsub(first,"은$|는$|이$|가$|들이$")
gsub("은$|는$|이$|가$|들이$","",first)
first <- readLines('김대중.txt',encoding='UTF-8')
first <- unlist(str_split(gsub("[[:punct:]]", "", first)," "))
first
gsub("은$|는$|이$|가$|들이$","",first)
gsub("은$|는$|이$|가$|들이$|.니다$|.니다$|을$|를$","",first)
first <- readLines('김대중.txt',encoding='UTF-8')
first <- unlist(str_split(gsub("[[:punct:]]", "", first[which(first!="")])," "))
first
gsub("은$|는$|이$|가$|들이$|.니다$|.니다$|을$|를$","",first)
gsub("은$|는$|이$|가$|들이$|.니다$|.니다$|을$|를$|으로$|의$|도$","",first)
gsub("(은|는|이|가|들이|.니다|.니다|을|를|으로|의|도)$","",first)
gsub("(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("(은|는|^[같]이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("(은|는|[^같]이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("([^같]은|는|[^같]이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
first
gsub("(^[가-힣]..은|는|^[가-힣]..이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("(^(?!같).은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("(^(?=.*같).은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("(?=.*같)(.은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("^(?=.*같)(.은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("(.은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub("(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
first
gsub("(?!.*같)(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$","",first)
gsub('(?!.*같)(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$',"",first)
gsub('^(?!.*같)(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$',"",first)
gsub('^(?!.*같)(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)',"",first)
doc_list <- list()
for(i in length(file_ls[1:2])){
first <- readLines(file_ls[i],encoding='UTF-8')
first <- unlist(str_split(gsub("[[:punct:]]", "", first[which(first!="")])," "))
first <- gsub('(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$',"",first)
doc_list[[i]] <- first
#tdm <- DocumentTermMatrix(first, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
#Encoding(tdm$dimnames$Terms)
}
doc_list <- list()
for(i in length(file_ls[1:2])){
first <- readLines(file_ls[i],encoding='UTF-8')
first <- unlist(str_split(gsub("[[:punct:]]", "", first[which(first!="")])," "))
first <- gsub('(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$',"",first)
doc_list[[i]] <- first
#tdm <- DocumentTermMatrix(first, control=list(wordLengths=c(2,Inf),
#weighting = function(x) weightTfIdf(x, normalize = TRUE)))
#Encoding(tdm$dimnames$Terms)
}
doc_list <- list()
for(i in length(file_ls[1:2])){
first <- readLines(file_ls[i],encoding='UTF-8')
first <- unlist(str_split(gsub("[[:punct:]]", "", first[which(first!="")])," "))
first <- gsub('(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$',"",first)
doc_list[[i]] <- first
#tdm <- DocumentTermMatrix(first, control=list(wordLengths=c(2,Inf),
#weighting = function(x) weightTfIdf(x, normalize = TRUE)))
#Encoding(tdm$dimnames$Terms)
}
doc_list
tdm <- DocumentTermMatrix(first, control=list(wordLengths=c(2,Inf),
#weighting = function(x) weightTfIdf(x, normalize = TRUE)))
)
()
(
)))))
tdm <- DocumentTermMatrix(first, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
test <- VCorpus(DirSource(my.text.location))
tdm <- DocumentTermMatrix(test, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
as.matrix(tdm)[1:5,1:5]
test <- VCorpus(DirSource(my.text.location,pattern = 'txt'))
test
tdm <- DocumentTermMatrix(test, control=list(wordLengths=c(2,Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
tdm
as.matrix(tdm)[1:5,1:5]
test <- VCorpus(DirSource(my.text.location,pattern = 'txt'))
str(test[[1]])
test <- VCorpus(DirSource(my.text.location,pattern = 'txt'))
str(test[[1]])
test <- VCorpus(DirSource(my.text.location,pattern = 'txt'))
str(test[[1]])
str(test[[2]])
str(test[[17]])
seq_along(test)
for(i in seq_along(test)){
test[[i]]$content <- paste(test[[i]]$content, collapse=" ") #한줄로
}
test[[1]]$content
test <- tm_map(test, removePunctuation)
test
test[[1]]$content
tm_map(test,removeWords,stopwords('(은|는|이|가|들이|.니다|.겠습니다|을|를|으로|의|도|에)$'))
test <- tm_map(test, stripWhitespace)
test
library(NIADic)
useNIADic()
extractNoun(test[[1]]$content)
noun_list <- list()
noun_list <- list()
for(i in seq_along(test)){
test[[i]]$content <- gsub('읍니다$','습니다',test[[i]]$content)
nouns <- extractNoun(test[[i]]$content)
nouns <- nouns[nchar(nouns) >= 2]
noun_list[[i]]$content <- paste(nouns, collapse=" ")
}
test[[i]]$content <- gsub('읍니다$','습니다',test[[i]]$content)
nouns <- extractNoun(test[[i]]$content)
nouns <- nouns[nchar(nouns) >= 2]
noun_list[[i]]$content <- paste(nouns, collapse=" ")
noun_list <- test
for(i in seq_along(test)){
test[[i]]$content <- gsub('읍니다$','습니다',test[[i]]$content)
nouns <- extractNoun(test[[i]]$content)
nouns <- nouns[nchar(nouns) >= 2]
noun_list[[i]]$content <- paste(nouns, collapse=" ")
}
test_tdm <- DocumentTermMatrix(test, control=list(tokenize="scan", wordLengths=c(2, Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
test_tdm[1:5,1:5]
as.matrix(test_tdm)[1:5,1:5]
test[[i]]$content
nouns
paste(nouns, collapse=" ")
as.matrix(test_tdm)[1:5,1:5]
inspect(test_tdm)
test_tdm <- DocumentTermMatrix(noun_list, control=list(tokenize="scan", wordLengths=c(2, Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
#plan_tdm_1 <- TermDocumentMatrix(plans, control=list(wordLengths=c(2, 7)))
inspect(test_tdm)
wordFreq <- slam::row_sums(test_tdm)
wordFreq <- sort(wordFreq, decreasing=TRUE)
wordFreq
wordFreq <- slam::col_sums(test_tdm)
wordFreq
wordFreq[1]
names(wordFreq[1])
gsub('`','',names(wordFreq[1]))
gsub('‘','',names(wordFreq[1]))
gsub('‘|’','',names(wordFreq[1]))
noun_list <- test
for(i in seq_along(test)){
test[[i]]$content <- gsub('읍니다$','습니다',test[[i]]$content)
test[[i]]$content <- gsub('‘|’','',test[[i]]$content)
nouns <- extractNoun(test[[i]]$content)
nouns <- nouns[nchar(nouns) >= 2]
noun_list[[i]]$content <- paste(nouns, collapse=" ")
}
test_tdm <- DocumentTermMatrix(noun_list, control=list(tokenize="scan", wordLengths=c(2, Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
wordFreq <- slam::col_sums(test_tdm)
wordFreq
noun_list <- test
for(i in seq_along(test)){
test[[i]]$content <- gsub('읍니다$','습니다',test[[i]]$content)
test[[i]]$content <- gsub('‘|’|「|」|『','',test[[i]]$content)
nouns <- extractNoun(test[[i]]$content)
nouns <- nouns[nchar(nouns) >= 2]
noun_list[[i]]$content <- paste(nouns, collapse=" ")
}
test_tdm <- DocumentTermMatrix(noun_list, control=list(tokenize="scan", wordLengths=c(2, Inf),
weighting = function(x) weightTfIdf(x, normalize = TRUE)))
wordFreq <- slam::col_sums(test_tdm)
wordFreq <- sort(wordFreq, decreasing=TRUE)
wordFreq
wordFreq <- sort(wordFreq, decreasing=TRUE)[1:100]
library(wordcloud)
pal <- brewer.pal(8,"Dark2")
w <- names(wordFreq)
wordcloud(words=w, freq=wordFreq,
min.freq=3, random.order=F,
random.color=T, colors=pal)
install.packages("wordcloud2")
wordcloud2(wordFreq, figPath = "C:/Users/DKU/Desktop/presi.jpg")
library(wordcloud2)
wordcloud2(wordFreq, figPath = "C:/Users/DKU/Desktop/presi.jpg")
wordcloud2(data.frame(w,wordFreq), figPath = "C:/Users/DKU/Desktop/presi.jpg")
data.frame(w,wordFreq)
wordcloud2(data=data.frame(w,wordFreq),fontFamily = '나눔바른고딕')
figPath = system.file("C:/Users/DKU/Desktop/presi.jpg",package = "wordcloud2")
wordcloud2(data.frame(w,wordFreq), figPath = figPath, size = 1.5,color = "skyblue")
wordcloud2(data.frame(w,wordFreq), figPath = figPath, size = 1.5,color = "skyblue")
figPath = system.file("C:/Users/DKU/Desktop/presi.JPEG",package = "wordcloud2")
wordcloud2(data.frame(w,wordFreq), figPath = figPath, size = 1.5,color = "skyblue")
figPath = system.file("C:/Users/DKU/Desktop/presi.jpg",package = "wordcloud2")
system.file("C:/Users/DKU/Desktop/presi.jpg",package = "wordcloud2")
library(devtools)
devtools::install_github("lchiffon/wordcloud2")
wordcloud2(data.frame(w,wordFreq), figPath = "C:/Users/DKU/Desktop/presi.jpg")
